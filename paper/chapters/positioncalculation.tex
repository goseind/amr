\section{Distance and position calculation}
\label{Distance and position calculation}
\subsection{Requirements}
In the next steps we have considered the position calculation of objects. A relatively accurate position would give us the possibility to approach objects relatively accurately and still not lose orientation in case of incorrect detection. In addition, delays can be calculated out.

For the implementation we wanted to use the incoming images. This would not require an additional sensor and we can continue to use the object recognition directly.

\subsection{Methods and failures}
For the implementation we tried several approaches. We distinguished between the \textit{x}-axis calculation and the \textit{y}-axis calculation. For simplification, we always considered the camera of the robot as the origin of the coordinate system and the objects outside changed their position with the movement of the robot. The \textit{y}-axis represented the right and left position of an object from the point of view of the robot. The \textit{x}-axis in turn represented the positive forward direction. Since we assume a flat floor and the robot cannot change the height, we see the Z-axis as irrelevant.

The units for the axes have been calculated in millimeters to simplify control.
For testing purposes, we took several pictures of the ball in several positions and
evaluated them with different methods.

\subsubsection{Horizon Approach}
The idea behind the approach is that nothing can exceed the horizon and thus there is a fixed scaling for the distance, which approaches the horizon asymptotically exponentially.
If the distance from the camera to the beginning of the image is known, this can be used to infer the distance to the object. For this you have to take the lowest position of the object and the relation of the position to the horizon. Then you can insert the relation into the function for the increasing distance which should be similar to this:

\begin{figure}[htbp]
\centering
\includesvg[width=\linewidth,inkscapelatex=false]{images/implementation/exponentialAsymptote.svg}
\caption{the y-axis gives us the distance and the
x-axis the position of the object between lense (0) and horizon(asymptote)}
\label{fig:exponentialAsymptoteGraph}
\end{figure}

The problem with the approach, however, was that the calculation depended heavily
on the distance from camera to image start and this was difficult to measure. In
addition, the results became inaccurate quite quickly, since the exponential change
with increasing distance also increased the inaccuracy exponentially.

\subsubsection{Focal-length Approach}
In this approach, we used the focal length and sensor size of the camera. Considering the real object and the projection, which is thrown through the lens onto the sensor, the rays allow to apply the intercept theorem. Distortions due to the lens were neglected for the time being. With the second intercept theorem you can conclude that the relation between the focal length of the camera and the size of the object on the sensor must be the same relation as the relation between the distance from the sensor to the object to the size of the object. Thus 3 constants
would be necessary, which must be known:
\begin{itemize}
\item Focal length sensor
\item Sensor size
\item Size of the object
\end{itemize}

Assuming that $f$ is the focal length of the camera in mm, $h_s$ the height of the object presented on the sensor in mm and $h_o$ the height of the object in the real world in mm, then the formula to calculate the distance in forward direction from the camera to the object is

\begin{align}
    x = \frac{f}{h_s} \cdot h_o
\end{align}

The size of the object on the sensor can be determined simply by the relative size of the object on the image in relation to the image itself multiplied by the sensor size.
The advantage of this procedure was that we got back quite exact values
independent of the camera height. Even at greater distances, the deviations were still manageable, which was more than sufficient for our scenario.
Disadvantage, however, was that we would thus be bound to fixed objects, since the size could no longer be changed.

\subsubsection{Relation Approach}
Here we try to determine the distance to the center of the image by the relative
position of the ball. For this again the size or width of the object is necessary as a relation. First we measure the width and the distance to the center of the detected box of the object. In the next step we look how many times the object fits into the distance to the center of the image. The amount will then be multiplied by the real world size of the object. The result will be the distance from the center to the object.

With this method, we have managed to obtain fairly accurate measurements, with
deviations of just about 3cm. More details on section below. Moreover, only one constant is necessary. Also disadvantage here again is that the size of the object must be known and therefore the object can be exchanged by another object only with difficulty.

\subsection{Result}
Since our camera is not always at the same position due to modifications and thus our first approach would no longer work and lead to large inaccuracies. 
On the other hand, the Focal Length and Relation Approach were the most suitable for our scenario. For our purposes we have chosen tennis balls, which are standardized by the ITF and have only slight variations in size \autocite{Int_TennisFederation}. In our case, we assumed a diameter of 6.7 cm. In order to test the procedures properly, we took several pictures from the robot, placing the ball at different positions and moving the camera vertically. As test positions we took:
\begin{itemize}
\item X: 20cm, Y: 0cm
\item X: 40cm, Y: 0cm
\item X: 60cm, Y: 0cm
\item X: 100cm, Y: 0cm
\item X: 100cm, Y: -20cm
\end{itemize}

For each image, we recorded the camera once in the lower anchorage of the robot and once in the upper anchorage of the robot. Since we use the rpi-wwcam2 the specifications were given by Prof. Dr. Ihme \autocite{Ihme_Latenzarme}. For the further sensor specifications informations like sensor size can also be found on the Internet, e.g. in the Reichelt e-shop \autocite{Reichelt_RPIWWCAM2}. However, the sensor size proves to be difficult, which is unfortunately specified in inches in most cases. For this, a table is needed to find out the height and width of the sensor, as provided by the website Photoreview.com \autocite{UnravellingSensor}. The following tables \autoref{table:positionCalculationHigh} and \autoref{table:positionCalculationLow} showing the results. For the X-Position the Focal Length Approach was used and for the Y-Position the Relation Approach.

\begin{table}[h]
  \caption{Results of position calculation with high camera.}
  \label{table:positionCalculationHigh}
  \renewcommand{\arraystretch}{1.2}
  \centering
  \sffamily
  \begin{footnotesize}
    \begin{tabularx}{0.58\linewidth}{l L L L L L L L}
      \toprule
      \textbf{Picture} & \textbf{X (in cm)} & \textbf{Y (in cm)} \\
      \midrule
        (20, 0)  & 23.47   & -1.71      \\
        (40, 0)  & 39.49   & -2.33      \\
        (60, 0)  & 56.89   & -2.1       \\
        (100, 0) & 90.72   & -2.8       \\
        (100, -20) & 95.91 & -21.34     \\
      \bottomrule
    \end{tabularx}
  \end{footnotesize}
  \rmfamily
\end{table}

\begin{table}[h]
  \caption{Results of position calculation with low camera.}
  \label{table:positionCalculationLow}
  \renewcommand{\arraystretch}{1.2}
  \centering
  \sffamily
  \begin{footnotesize}
    \begin{tabularx}{0.58\linewidth}{l L L L L L L L}
      \toprule
      \textbf{Picture} & \textbf{X (in cm)} & \textbf{Y (in cm)} \\
      \midrule
        (20, 0)    & 23.15  & -2.66        \\
        (40, 0)    & 38.58  & -0.8         \\
        (60, 0)    & 56.89  & -1.6         \\
        (100, 0)  & 88.34  & -1.76        \\
        (100, -20) & 98.73  & -22.86       \\
      \bottomrule
    \end{tabularx}
  \end{footnotesize}
  \rmfamily
\end{table}


Both tables show only minimal deviations of $\pm$3cm at the beginning. Only with further distance deviations of about 10cm resulted. In addition, the lens of the camera has a strong fisheye in the center, which would explain why the position (100, 0) has a much stronger deviation than the position (100, -20). However, the latter can be neglected in the deviation, since theoretically the position is always recalculated when approaching. For larger problems, however, the distortion can still be calculated using a checkerboard pattern.

The deviations were most likely also caused by measurement inaccuracies when measuring the ball and the distance measurements for the test images, since these were carried out in a room and with measurement materials that were not designed for this purpose.

Nevertheless, a complete implementation of both methods and thus a real test procedure was not achieved, because in addition to the exact position recognition, an extended navigation procedure is also necessary, which was not possible to implement due to time constraints.
