\section{Conclusion}

Over the course of this project, we managed to create a system that is able to detect sports balls, maneuver towards them, collect them, detect the destination and bring the sports ball to the destination. To achieve this, we used a TurtleBot3 Burger as a mobile manipulation plattform in combination with \ac{ROS} and a docker container setup that allows for an easy reproduction and transfer of the system. We managed to find a fast and accurate object detection architecture that is able to process images at a fast enough pace that the detections can be used for navigating the TurtleBot. The system runs partially on the TurtleBot and partially on a laptop where the images are processed. This allowed for a faster processing of the images. We furthermore implemented a calculation of the distance from the camera to the detected objects based solely on the image received from the camera, although it was not used by the navigation in the end.

Although we ran into some problems regarding the technical setup and the communication between the TurtleBot and the laptop and the delay due to the distributed system, the final result managed to meet our expectations. The system is able to execute the desired tasks rather reliably. 

There are some areas left that could be looked further into in the future: The distance calculation could be used to steer the TurtleBot more precisely and change the speed depending on the distance from the target for smoother navigation. . Also, a detection model could be trained specifically for the desired tasks to enhance the detection behavior even more.

In the future an absolute position calculation as described in chapter \ref{Distance and position calculation} could be used to maneuver around obstacles. With this approach, positions could be recognized and remembered quite exactly. Problems like the object moving out of sight, delayed informations due to network problems or the object is not recognized all the time could be fixed by this.  However, it would be necessary to measure the robot's track while driving in order to update the positions of the surrounding objects if they are not recognizable. A dimension of the wheels and the rotational speed of the wheels would be one way to make this happen. In addition, an extended recognition of obstacles and driving around them would be possible. For this purpose, a self-trained neural network that is not limited to the given classes of the model used in this project would be advantageous.

The position detection can also be improved by calculating the distortion caused by the fisheye to avoid larger deviations in the center of the image. In addition, position recognition using \ac{AI} or multiple images from multiple perspectives instead of just one image would be conceivable in order to find calculate the position regardless of the size of the objects.